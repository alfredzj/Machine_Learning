{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> ***Score: 100 (Very good!)*** </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MTH 9899 Machine Learning Assignment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 \n",
    "In class, we spoke about the time complexity for multiplying matrices. Ignoring more sophisticated algorithms, like the Strassen algorithm, multiplying an $a × b$ matrix by a $b × c$ matrix takes $O(abc)$. As we did in class, please work out the time complexity of computing a naive K-Fold Cross Validation Ridge Regression on an $N × F$ input matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='blue'>Solution</font>\n",
    "A k-fold cross validation ridge regression means that we fit our model k times on different data set.\n",
    "For each time, we use the following formula to calculate the parameters.\n",
    "$$\n",
    "\\hat{\\beta}^R = (X^T \\, X + \\lambda \\, I)^{-1} \\, X^T \\, Y\n",
    "$$\n",
    "where $X$ is a $N × F$ matrix.\n",
    "\n",
    "According to the problem, we know that\n",
    "\n",
    "  | matrix computation (step by step)           | time complexity|\n",
    "  | :-----------------------------------------: | :-----------:  |\n",
    "  | $X^T \\, X$                                  | $O(N \\, F^2)$  |\n",
    "  | $\\lambda \\, I$                              | $O(F)$         |           \n",
    "  | $(X^T \\, X + \\lambda \\, I)^{-1}$            | $O(F^3)$       |\n",
    "  | $(X^T \\, X + \\lambda \\, I)^{-1} X^T$        | $O(F^2 \\, N)$  |\n",
    "  | $(X^T \\, X + \\lambda \\, I)^{-1} X^T Y$      | $O(F \\, N)$  |\n",
    "  \n",
    "So the time complexity is $O(N \\, F^2 + F + F^3 + F^2 \\, N + F \\, N)$ or $O(max(F^3, N \\, F^2  ))$\n",
    "\n",
    "Since we will run regression k times, thus the total time complexity is $O(K(2 N \\, F^2 + F + F^3 + F \\, N))$ or $O(max(k \\, F^3, k \\, N \\, F^2  ))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    "We can be more efficient. In particular, we don’t have to compute $(X^T X )^{-1}$ completely each time. In particular, if you break up $X$ into $K$ chunks, there is a faster way.\n",
    "$$\n",
    "X = 2 \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "X = \\left[\n",
    "    \\begin{matrix}\n",
    "     X_1 \\\\ X_2 \\\\ ... \\\\ X_k \n",
    "    \\end{matrix}\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^T X = \\left[\n",
    "        \\begin{matrix}\n",
    "        X_1^T & X_2^T & ... & X_k^T \n",
    "        \\end{matrix}\n",
    "        \\right] \n",
    "        \\left[\n",
    "        \\begin{matrix}\n",
    "        X_1 \\\\ X_2 \\\\ ... \\\\ X_k \n",
    "        \\end{matrix}\n",
    "        \\right] \n",
    "$$\n",
    "• Define $X_{-i}$ as $X$ with the ith fold omitted. Given these hints, write a description of how you can efficiently compute $X_{-i}^T \\, X_{-i}$ for all K folds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='blue'>Solution</font>\n",
    "\n",
    "Using block matrix multiplication we know that\n",
    "\n",
    "$$\n",
    "X^T X = \\left[\n",
    "        \\begin{matrix}\n",
    "        X_1^T & X_2^T & ... & X_k^T \n",
    "        \\end{matrix}\n",
    "        \\right] \n",
    "        \\left[\n",
    "        \\begin{matrix}\n",
    "        X_1 \\\\ X_2 \\\\ ... \\\\ X_k \n",
    "        \\end{matrix}\n",
    "        \\right] \n",
    "       =\\left[\n",
    "        \\begin{matrix}\n",
    "        X_1^T \\, X_1 + X_2^T \\, X_2 + ... + X_k^T \\, X_k \n",
    "        \\end{matrix}\n",
    "        \\right] \n",
    "$$\n",
    "\n",
    "Here is an efficient way to calculate $X_{-i}^T \\, X_{-i}$:\n",
    "\n",
    "- calculate $X_1^T \\, X_1 + X_2^T \\, X_2 + ... + X_k^T \\, X_k$, mark it as $T$\n",
    "\n",
    "- then $X_{-i}^T \\, X_{-i} = T - X_i^T \\, X_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3\n",
    "Implement the algoirthms discussed in a Jupyter Notebook.\n",
    "\n",
    "* Using what you learned in Problem 2, implement 2 versions of Ridge Regression in the Python template shown below. One should be the slower naive algorithm, the other should be the faster version derived in Problem 2. Don’t use any external math packages (other than NumPy).\n",
    "\n",
    "* Test them. Generate random datasets with varying numbers of rows (anything from 1000 rows to 1,000,000) for 5 and 50 columns. Test both algos with 10 reasonable lambda values, and plot the time it takes to compute both versions as a function of N.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "def generate_test_data(n, f):\n",
    "    np.random.seed(1)\n",
    "    true_betas = np.random.randn(f)\n",
    "    X = np.random.randn(n,f)\n",
    "    Y = np.random.randn(n) + X.dot(true_betas)\n",
    "    return (X,Y)\n",
    "\n",
    "def naive_ridge_cv(X, Y, num_folds , lambdas):\n",
    "    ''' Implements a naive(ie slow) Ridge Regression of X against Y. \n",
    "    It will take in a list of suggested lambda values and return back the lambda and\n",
    "    betas that generates minimum mean squared error.\n",
    "    \n",
    "    Parameters\n",
    "    −−−−−−−−−−\n",
    "    X : numpy ndarray\n",
    "        The independent variables, structured as (sample x features)\n",
    "    Y: numpy ndarray\n",
    "        The dependent variable, (samples x 1 )\n",
    "    num_folds: int\n",
    "        The number of folds to use for crossvalidation\n",
    "    lambdas : numpy ndarray\n",
    "        An array of lambda values to test\n",
    "    \n",
    "    Returns\n",
    "    −−−−−−−\n",
    "    lambda_star: float\n",
    "        The lambda value that represents the min MSE\n",
    "    beta_star: numpy ndarray\n",
    "        The optimal betas\n",
    "    '''\n",
    "    return (lambdas[0], np.repeat(0, X.shape[1]))\n",
    "\n",
    "\n",
    "def fast_ridge_cv(X, Y, num_folds, lambdas):\n",
    "    ''' Implements a naive(ie slow) Ridge Regression of X against Y. \n",
    "    It will take in a list of suggested lambda values and return back the lambda and\n",
    "    betas that generates minimum mean squared error.\n",
    "    \n",
    "    Parameters\n",
    "    −−−−−−−−−−\n",
    "    X : numpy ndarray\n",
    "        The independent variables, structured as (sample x features)\n",
    "    Y: numpy ndarray\n",
    "        The dependent variable, (samples x 1 )\n",
    "    num_folds: int\n",
    "        The number of folds to use for crossvalidation\n",
    "    lambdas : numpy ndarray\n",
    "        An array of lambda values to test\n",
    "    \n",
    "    Returns\n",
    "    −−−−−−−\n",
    "    lambda_star: float\n",
    "        The lambda value that represents the min MSE\n",
    "    beta_star: numpy ndarray\n",
    "        The optimal betas\n",
    "    '''\n",
    "    return (lambdas[0], np.repeat(0, X.shape[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='blue'>Solution</font>\n",
    "\n",
    "The following is the realizations of naive ridge regression and fast ridge regression and relateds test on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def generate_test_data(n, f):\n",
    "    '''parameter:\n",
    "        n: num of samples\n",
    "        f: num of features\n",
    "       return:\n",
    "        X: generated independent variables\n",
    "        Y: generated dependent variables\n",
    "        true_betas: true beta for test in the future\n",
    "    '''  \n",
    "    np.random.seed(1)\n",
    "    true_betas = np.random.randn(f)\n",
    "    X = np.random.randn(n,f)\n",
    "    Y = np.random.randn(n) + X.dot(true_betas)\n",
    "    return (X,Y)\n",
    "\n",
    "\n",
    "def ridge_regression(X, Y, lambda_):\n",
    "    '''naive way to solve ridge regression\n",
    "    '''\n",
    "    return np.linalg.inv(np.transpose(X).dot(X)+lambda_*(np.identity(len(X[0])))).dot(np.transpose(X)).dot(Y)\n",
    "\n",
    "\n",
    "def calculate_cv(X, Y, beta_):\n",
    "    '''calculate the cost of the regression\n",
    "    '''\n",
    "    return np.linalg.norm(Y - X.dot(beta_))\n",
    "\n",
    "\n",
    "def fn_timer(f):\n",
    "    '''running time tracker\n",
    "    '''\n",
    "    @wraps(f)\n",
    "    def wrapper(*args, **kwds):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwds)\n",
    "        elapsed = time.time() - start\n",
    "        print (\"{0} takes {1} seconds to finish\".format(f.__name__, elapsed))\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@ fn_timer\n",
    "def naive_ridge_cv(X, Y, num_folds , lambdas):\n",
    "    '''parameter:\n",
    "        X: The independent variables\n",
    "        Y: The dependent variable\n",
    "        num_folds: the number of folds to use for crossvalidation\n",
    "        lambdas : an array of lambda values to test\n",
    "       return:\n",
    "        lambda_star: the lambda value that represents the min MSE\n",
    "        estimated_beta: the optimal betas\n",
    "    '''  \n",
    "    x, y = X, Y\n",
    "    # random partition by sequence\n",
    "    x_split = np.array_split(x, num_folds)\n",
    "    y_split = np.array_split(y, num_folds)  \n",
    "    # compute the training data and cross validationn data in advance\n",
    "    x_cv_total, y_cv_total, x_train_total, y_train_total = [], [], [], []\n",
    "    for j in range(len(x_split)):\n",
    "        x_split_cy, y_split_cy = x_split, y_split\n",
    "        x_cv, y_cv = x_split_cy[j], y_split_cy[j]\n",
    "        x_train = np.concatenate(np.delete(x_split_cy, j, 0))\n",
    "        y_train = np.concatenate(np.delete(y_split_cy, j, 0))\n",
    "        x_cv_total.append(x_cv)\n",
    "        y_cv_total.append(y_cv)\n",
    "        x_train_total.append(x_train)\n",
    "        y_train_total.append(y_train)\n",
    "      \n",
    "    # using crossvalidation to choose lambda\n",
    "    lambda_star, mse = 99999., 99999.\n",
    "    for lambda_ in lambdas:\n",
    "        mse_td, beta_td = 0., None\n",
    "        for j in range(len(x_split)):\n",
    "            beta_td = ridge_regression(x_train_total[j], y_train_total[j], lambda_)\n",
    "            mse_td += calculate_cv(x_cv_total[j], y_cv_total[j], beta_td)\n",
    "        if mse_td < mse:\n",
    "            mse = mse_td\n",
    "            lambda_star = lambda_ \n",
    "        \n",
    "#         print (lambda_star, mse)\n",
    "    \n",
    "    estimated_beta = ridge_regression(x, y, lambda_star)\n",
    "    return (lambda_star, estimated_beta)\n",
    "        \n",
    "       \n",
    "@ fn_timer\n",
    "def fast_ridge_cv(X, Y, num_folds, lambdas):\n",
    "    '''parameter:\n",
    "    X: The independent variables\n",
    "    Y: The dependent variable\n",
    "    num_folds: the number of folds to use for crossvalidation\n",
    "    lambdas : an array of lambda values to test\n",
    "   return:\n",
    "    lambda_star: the lambda value that represents the min MSE\n",
    "    estimated_beta: the optimal betas\n",
    "    '''  \n",
    "    x, y = X, Y\n",
    "    # random partition by sequence\n",
    "    x_split = np.array_split(x, num_folds)\n",
    "    y_split = np.array_split(y, num_folds)\n",
    "    # using crossvalidation to choose lambda\n",
    "    lambda_star, mse = 99999., 99999.\n",
    "    # matrix multiplication optimazation(1)    \n",
    "    xTx_i = [np.transpose(x_split[i]).dot(x_split[i]) for i in range(num_folds)] \n",
    "    xTy_i = [np.transpose(x_split[i]).dot(y_split[i]) for i in range(num_folds)]\n",
    "    xTx_total = sum(xTx_i)\n",
    "    xTy_total = sum(xTy_i) \n",
    "    \n",
    "    # compute the training data and cross validationn data in advance\n",
    "    x_cv_total, y_cv_total = [], []\n",
    "    for j in range(len(x_split)):\n",
    "        x_split_cy, y_split_cy = x_split, y_split\n",
    "        x_cv, y_cv = x_split_cy[j], y_split_cy[j]\n",
    "        x_cv_total.append(x_cv)\n",
    "        y_cv_total.append(y_cv)\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        mse_td, beta_td = 0., None\n",
    "        for j in range(len(x_split)):\n",
    "            # matrix multiplication optimazation(2)\n",
    "            beta_td = np.linalg.inv(xTx_total - xTx_i[j]+lambda_*(np.identity(len(xTx_total[0])))).dot(xTy_total - xTy_i[j])\n",
    "            mse_td += calculate_cv(x_cv_total[j], y_cv_total[j], beta_td) \n",
    "        if mse_td < mse:\n",
    "            mse = mse_td\n",
    "            lambda_star = lambda_\n",
    "        \n",
    "#         print (lambda_star, mse)\n",
    "    \n",
    "    estimated_beta = np.linalg.inv(xTx_total+lambda_star*(np.identity(len(xTx_total[0])))).dot(xTy_total)\n",
    "    return (lambda_star, estimated_beta)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive_ridge_cv takes 61.17366027832031 seconds to finish\n",
      "\n",
      "lambda:\n",
      " 2.4 \n",
      "\n",
      " beta:\n",
      " [ 1.62500495 -0.61263898 -0.52700233 -1.07173175  0.86477654 -2.30189444\n",
      "  1.74481849 -0.7620215   0.31874983 -0.24829842  1.46024627 -2.06003245\n",
      " -0.32139215 -0.38415491  1.13226438 -1.10140153 -0.17286894 -0.87669016\n",
      "  0.04110974  0.58388025 -1.09999676  1.14314706  0.9012994   0.5032255\n",
      "  0.89891579 -0.68190816 -0.12514973 -0.93583959 -0.26826734  0.53060164\n",
      " -0.69189235 -0.39715116 -0.68712863 -0.84467937 -0.67284298 -0.01165828\n",
      " -1.11683285  0.23375404  1.65905095  0.74219659 -0.19250049 -0.88831489\n",
      " -0.74660163  1.69347187  0.04935637 -0.6366989   0.19042595  2.10012228\n",
      "  0.12096707  0.61806899] \n",
      "\n",
      "-----------------------------------------\n",
      "\n",
      "fast_ridge_cv takes 0.656611442565918 seconds to finish\n",
      "\n",
      "lambda:\n",
      " 2.4 \n",
      "\n",
      " beta:\n",
      " [ 1.62500495 -0.61263898 -0.52700233 -1.07173175  0.86477654 -2.30189444\n",
      "  1.74481849 -0.7620215   0.31874983 -0.24829842  1.46024627 -2.06003245\n",
      " -0.32139215 -0.38415491  1.13226438 -1.10140153 -0.17286894 -0.87669016\n",
      "  0.04110974  0.58388025 -1.09999676  1.14314706  0.9012994   0.5032255\n",
      "  0.89891579 -0.68190816 -0.12514973 -0.93583959 -0.26826734  0.53060164\n",
      " -0.69189235 -0.39715116 -0.68712863 -0.84467937 -0.67284298 -0.01165828\n",
      " -1.11683285  0.23375404  1.65905095  0.74219659 -0.19250049 -0.88831489\n",
      " -0.74660163  1.69347187  0.04935637 -0.6366989   0.19042595  2.10012228\n",
      "  0.12096707  0.61806899] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''test code (1)\n",
    "'''\n",
    "# generate test data\n",
    "X_test, Y_test = generate_test_data(1000000, 50)\n",
    "# enter the num of folds\n",
    "num_folds = 10\n",
    "# lambda to be determine\n",
    "lambdas = [0., 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7]\n",
    "# ridge regression\n",
    "(lambda_star1, estimated_beta1) = naive_ridge_cv(X_test, Y_test, num_folds, lambdas)\n",
    "print ('\\nlambda:\\n',lambda_star1,'\\n\\n', 'beta:\\n',estimated_beta1,'\\n\\n-----------------------------------------\\n')\n",
    "(lambda_star2, estimated_beta2) = fast_ridge_cv(X_test, Y_test, num_folds, lambdas)\n",
    "print ('\\nlambda:\\n',lambda_star2,'\\n\\n', 'beta:\\n',estimated_beta2,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive_ridge_cv takes 0.2675759792327881 seconds to finish\n",
      "\n",
      "lambda:\n",
      " 0.6 \n",
      "\n",
      " beta:\n",
      " [ 1.63777281 -0.63162314 -0.52181925 -1.06637185  0.86674778 -2.3103367\n",
      "  1.75307429 -0.77324949  0.30982223 -0.23594614  1.44870832 -2.05687869\n",
      " -0.32409243 -0.38952282  1.12445754 -1.09676737 -0.17960246 -0.87814592\n",
      "  0.05723144  0.58636737 -1.09320444  1.12778275  0.89928735  0.50131813\n",
      "  0.89744813] \n",
      "\n",
      "-----------------------------------------\n",
      "\n",
      "fast_ridge_cv takes 0.008680582046508789 seconds to finish\n",
      "\n",
      "lambda:\n",
      " 0.6 \n",
      "\n",
      " beta:\n",
      " [ 1.63777281 -0.63162314 -0.52181925 -1.06637185  0.86674778 -2.3103367\n",
      "  1.75307429 -0.77324949  0.30982223 -0.23594614  1.44870832 -2.05687869\n",
      " -0.32409243 -0.38952282  1.12445754 -1.09676737 -0.17960246 -0.87814592\n",
      "  0.05723144  0.58636737 -1.09320444  1.12778275  0.89928735  0.50131813\n",
      "  0.89744813] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''test code (2)\n",
    "'''\n",
    "# generate test data\n",
    "X_test, Y_test = generate_test_data(10000, 25)\n",
    "# enter the num of folds\n",
    "num_folds = 10\n",
    "# lambda to be determine\n",
    "lambdas = [0., 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7]\n",
    "# ridge regression\n",
    "(lambda_star1, estimated_beta1) = naive_ridge_cv(X_test, Y_test, num_folds, lambdas)\n",
    "print ('\\nlambda:\\n',lambda_star1,'\\n\\n', 'beta:\\n',estimated_beta1,'\\n\\n-----------------------------------------\\n')\n",
    "(lambda_star2, estimated_beta2) = fast_ridge_cv(X_test, Y_test, num_folds, lambdas)\n",
    "print ('\\nlambda:\\n',lambda_star2,'\\n\\n', 'beta:\\n',estimated_beta2,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive_ridge_cv takes 0.035222768783569336 seconds to finish\n",
      "\n",
      "lambda:\n",
      " 1.8 \n",
      "\n",
      " beta:\n",
      " [ 1.59911114 -0.67755839 -0.52105042 -1.0668743   0.83223323] \n",
      "\n",
      "-----------------------------------------\n",
      "\n",
      "fast_ridge_cv takes 0.00532221794128418 seconds to finish\n",
      "\n",
      "lambda:\n",
      " 1.8 \n",
      "\n",
      " beta:\n",
      " [ 1.59911114 -0.67755839 -0.52105042 -1.0668743   0.83223323] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''test code (3)\n",
    "'''\n",
    "# generate test data\n",
    "X_test, Y_test = generate_test_data(1000, 5)\n",
    "# enter the num of folds\n",
    "num_folds = 10\n",
    "# lambda to be determine\n",
    "lambdas = [0., 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7]\n",
    "# ridge regression\n",
    "(lambda_star1, estimated_beta1) = naive_ridge_cv(X_test, Y_test, num_folds, lambdas)\n",
    "print ('\\nlambda:\\n',lambda_star1,'\\n\\n', 'beta:\\n',estimated_beta1,'\\n\\n-----------------------------------------\\n')\n",
    "(lambda_star2, estimated_beta2) = fast_ridge_cv(X_test, Y_test, num_folds, lambdas)\n",
    "print ('\\nlambda:\\n',lambda_star2,'\\n\\n', 'beta:\\n',estimated_beta2,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Conclusion\n",
    "From above test results, we can see that both methods can choose the right $\\lambda$ and give correct estimation of $\\beta$. Moreover, the running time of <font color='blue'>fast_ridge_cv</font> takes less time than <font color='blue'>naive_ridge_cv</font>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
